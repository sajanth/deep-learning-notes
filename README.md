# Notes on Deep Learning 

While there has been an explosion of deep learning (DL) research in the last decade with successful applications across a wide variety of fields like computer vison, natural language processing, board games and natural sciences, a proper theoretical understanding explaining the observed effectivness of DL approaches is still an open question.

In this repo I am collecting some personal notes on exploring answers to questions like
* Why can over-parameterizied networks generalize?
* Why are gradient descent methods able to reliably find useful minimas in a high dimensional non-convex space?
* Why do vastly different network initializations lead to similair solutions after training?
* Can we expect from a proper mathematical theory of DL that it sheds light on learning and intelligent behaviour in biological system as well? 


