How do models learn and represent semantic concepts?

Some deep learning models (in particular when they have reached a critical scale in terms of number of parameters or training data size) seem to not just naively model surface level statistics (even though that is exactly what they are trained to do) but instead show emergent properties which require a deeper understanding of concepts - or at least it seems to me that they do. This is currently most salient in large language models like ChatGPT

![](../media/semantics/2022-12-05-22-34-22.png)
Looking at this example [...]

In these notes we try to explore and understand how semantic knowledge is represented mathematically in modern deep learning networks.

## Footnotes
1. Here we focus on language as they are currently the most salient examples but we will explore the question of visual understanding further below.
